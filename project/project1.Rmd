---
title: "Project 1"
author: "Ozioma Akahara (oaa847)"
date: "3/16/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```


## Introduction

The two data sets I used are "murder_2015_final" and "police_locals".The "murder_2015_final" data set (m15f) contained five variables (two character and three numeric), and it had eighty-three observations. After tidying, the number of variables remained five (three character and two numerical), and it had a hundred and sixty-six observations. The "police_locals" data set (pl) contained eight variables (one character and seven numeric), and it had seventy-five observations. After tidying, it had four variables (two character and two numerical), and it had four hundred and fifty observations. The "police_local" data set was acquired using calculations based on the U.S. Census, and this data set was interesting to me because it was compiled in response to the racially driven Ferguson protests that occurred in Missouri; the creators of the data set reasoned that if Ferguson had a huge racial gap in its population it should also be reflected in the police force assuming that they were locals. This reasoning caused the creators to compile a data set questioning the living situations of police in major cities. The "murder_2015_final" data set was acquired from the FBI crime data, this data was interesting to me because I was curious to see if force size (gotten from the police_locals data set) would have an effect on murders. I would expect that large police forces would have fewer murder cases or at least a negative change in murders between 2014 and 2015. 


```{R}
library(fivethirtyeight)
library(tidyverse)
write_csv(murder_2015_final, "m15f.csv")
getwd()
m15f <- read_csv("m15f.csv")
str(m15f)

write_csv(police_locals, "pl.csv")
getwd()
pl <- read_csv("pl.csv")
str(pl)

```



## Reshaping data sets

For both data sets, I noticed that there were several variables that could be grouped together under a column. So for the "pl" data set, I used "pivot_longer" to group the variables: all, white, non_white, black, hispanic and asian,  under a new variable that I called "race" and I put their values in a new variable called "perc_in" which means the percentages of the police depending on race that are locals in their city; this new tidy data set was termed "pl_tidy". Then for the "m15f" data set, I used "pivot_longer" to group the variables: murders_2014, murders_2015, under a new variable that I called "year" and I used the "separate function" to rename the observations in the column into "2014" and "2015"; this new tidy data set was termed "m15f_tidy". I noticed after tidying this data set that the variable "year" was numeric, but I wanted it to be a character variable so I used 'as.character' to make it one. 

```{R}
pl_tidy <- pl %>% pivot_longer(all:asian, names_to = "race", values_to = "perc_in")
str(pl_tidy)

m15f_tidy <- m15f %>% pivot_longer(3:4, names_to = "years", values_to = "total_murders") %>% separate(years,into = c(NA,"year"), sep=8, convert=T)
m15f_tidy$year <- as.character(m15f_tidy$year)
str(m15f_tidy)

```


## Joining data sets

I performed an inner join on the tidy versions of the two data sets; I joined these data set by "city". This resulted in eight variables and three hundred and twenty-four observations; the joined data set was termed "m_pl".I used anti join to find that there were a hundred and twelve observations dropped from the joined data set and this is because these observations didn't have a match in either of the original data sets. I used inner join specifically because I wanted to see whether force size had an impact on the occurrence of murders in states across USA, so I only needed observations with matches. However, these dropped observations could skew the results I get from my summary statistics (i.e. mean, percentile, and so on) and graph visualizations since a lot of data was removed.

```{R}
m15f_tidy %>% inner_join(pl_tidy, by = "city")->m_pl
str(m_pl)

m15f_tidy %>% distinct() %>% str()
m_pl %>% distinct() %>% str()
pl_tidy %>% distinct() %>% str()
m15f_tidy %>% anti_join(pl_tidy, by="city") %>% distinct()
```



## Summary statistics for joined data

Using summarise_all, I found that there are 27 unique cities and 21 unique states in the m_pl data set. Using mutate to add a column for murder percentile and arrange to sort in descending and ascending order, I found that Chicago was in the 100th percentile for number of murders and Seattle was in the 1st percentile for number of murders. Using mutate to add a column for force size percentile and arrange again to sort in ascending order, I found that Seattle ranged between the 13th and 15th percentile for force size and Chicago ranged between the 93rd and 96th percentile for force size. I would have expected that murder cases and force size would be inversely related but it doesn't seem to be the case. Using group_by to group by states, summarise to find the maximum change in murder total among the states and arrange to sort in descending order, I found that the state with the highest change in murder total was Maryland, which had an additional 133 murder cases in 2015 than in 2014; I also found that Massachusetts had the least change with 15 less murder cases in 2015 than in 2014.


```{R}
m_pl %>% filter(city=="Houston")
m_pl%>% filter(between(force_size,2000,3000))
m_pl %>% arrange(desc(city), change)
m_pl %>% select(city, total_murders, race, perc_in)
m_pl %>% select(city,total_murders)
m_pl %>% mutate(`murder_pctile` = ntile(total_murders,100))%>% arrange(desc(murder_pctile))
m_pl %>% mutate(`murder_pctile` = ntile(total_murders,100))%>% arrange(murder_pctile)
m_pl %>% mutate(`force_pctile` = ntile(force_size,100))%>% arrange(force_pctile)
m_pl %>% summarise_all(n_distinct)
m_pl %>% group_by(state) %>% summarise(mean_murders=mean(total_murders, na.rm = T))
m_pl %>% group_by(city)%>%  summarise(sd_murders=sd(total_murders, na.rm = T))
m_pl %>% summarize(min_force_size = min(force_size, na.rm=T))
m_pl %>% group_by(state, race) %>%  summarise(mean_perc_in=mean(perc_in, na.rm=T))
m_pl %>% group_by(state)%>% summarise(max_change=max(change, na.rm = T)) %>% arrange(desc(max_change))
m_pl %>% group_by(state)%>%summarise(n_distinct(city))


```


## Correlation

From the correlation heat map we can see that force_size and change have the least correlation; this means that the size of a police force is almost completely independent of the change in murder cases between 2014 and 2015. We can see that force_size and total-murders have the highest correlation; this means that the size of the number of murder cases is highly dependent on the size of the police force. As we have ascertained in the previous subsection, the size of the police force is possibly inversely related to number of murder cases. 

```{R}
m_pl_cor <- m_pl %>% select_if(is.numeric) %>% cor(use="pair")

m_pl_tidycor <- m_pl_cor %>% as.data.frame %>% rownames_to_column("var1") %>%  pivot_longer(-1,names_to="var2",values_to="correlation")

m_pl_tidycor

m_pl_tidycor%>%ggplot(aes(var1,var2,fill=correlation))+  geom_tile()+  scale_fill_gradient2(low="red",mid="white",high="blue")+
  geom_text(aes(label=round(correlation,2)),color = "black", size = 4)+ 
  theme(axis.text.x = element_text(angle = 90, hjust=1))+ 
  coord_fixed()

```


###Visualization using ggplot2

To visualize the relationship between total murder cases and force size by state, I created a scatter plot using ggplot2. In this scatter plot, I colored the points by state. From this scatter plot, we can see that there is a positive relationship between total murders and force size; as force size increases, so does the total murder case. Also, it is evident that New York has the largest force size, with approximately 32500 police officers. Also, we can see that Illinois has the highest number of murder cases, with both 2014 and 2015 having over 400 cases.

```{R}
library(ggplot2)
str(m_pl)
ggplot(m_pl, aes(force_size, total_murders)) + geom_point(size=2, aes(color = state)) +
  theme_grey()+
  ggtitle("Total Murders And Force Size According to States in U.S.A") + ylab("Total Murders") + xlab("Force Size")

```


To visualize the relationship between total murder cases and states by change in murder cases over 2014 and 2015, I created a bar chart using ggplot2. In this bar chart, I colored the bars by the "change" variable. From this bar chart, we can see that Maryland, New York, Michigan and Illinois had the highest number of murder cases. Also, Colorado, Massachusetts, Minnesota and Washington had the lowest murder cases. Furthermore, the lightness of the blue in the Maryland bar shows that there were over 100 more murder cases in 2015 compared to 2014. On the other hand, the darkness of the blue in the Massachusetts bar shows that there were less murder cases in 2015 compared to 2014.

```{R}
ggplot(m_pl, aes(state, total_murders, fill=change))+geom_bar(stat="summary", fun=mean)+
  geom_errorbar(stat="summary", fun.data=mean_se) + scale_x_discrete(limit = c("Arizona", "California", "Colorado", "D.C.", "Florida", "Georgia", "Illinois", "Indiana", "Louisiana", "Maryland", "Massachusetts", "Michigan", "Minnesota", "Missouri", "Nevada", "New York", "Ohio", "Pennsylvania", "Texas", "Washington", "Wisconsin"), labels = c("AZ","CA","CO", "DC", "FL", "GA", "IL", "IN", "LA", "MD", "MA", "MI", "MN", "MO", "NV", "NY", "OH", "PA", "TX", "WA", "WI"))+
  theme_minimal()+
  ggtitle("Murders by State in U.S.A According to Change in Murder Count between Years") + ylab("Total Murders") + xlab("State")

```



###k-means/PAM clustering

I decided to perform k-means/PAM clustering on total_murders and force_size. First, I ran the cluster package to prepare R for clustering. Next, I selected just total_murders and force_size and placed them in a new data frame which I save as "clust_dat". Then, I used the silhouette width method to select the number of clusters to proceed with and I did this by making an empty vector to contain the silhouette width and then I worked out the kmeans solution, got the silhouette widths and took the averages of them. Using ggplot, I visualized the silhouette widths and found the elbow was at k=2. Next, I set my number of clusters as 2 and saved them into the "kmeans1" data frame. Then, using mutate, I saved my cluster assignment as a column in the clust_dat and saved this as a new data frame called "kmeansclust". Lastly, I made a scatter plot of kmeansclust and colored the points by cluster. From the graph, we can see that cluster 1 ranges from about 20-210 total murders and about 100-10000 police officers within the force; cluster 2 ranges from about 240-480 total murders and about 2500-32500 police officers within the force.

```{R}
library(tidyverse)
library(cluster)

#select variables for clustering
clust_dat<-m_pl%>%dplyr::select(total_murders,force_size)

#use silhouette width to select number of clusters
sil_width<-vector()
for(i in 2:10){  
  kms <- kmeans(clust_dat,centers=i)  
  sil <- silhouette(kms$cluster,dist(clust_dat))
  sil_width[i]<-mean(sil[,3])
}

#visualize silhouette widths
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

#set number of clusters
kmeans1 <- clust_dat %>% scale %>% kmeans(2)
kmeans1

#save cluster assignment as a column in your data set
kmeansclust <- clust_dat %>% mutate(cluster=as.factor(kmeans1$cluster))

#graph showing final cluster assignment
kmeansclust %>% ggplot(aes(total_murders,force_size,color=cluster)) + geom_point()
```







